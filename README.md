# AI-Powered Logistics Assistant (Phase 1) - Modular Architecture

This project implements an AI-powered logistics assistant with a clean, modular architecture that automates dataset profiling, generates intelligent metadata, and provides comprehensive type conversion analysis.

## ğŸ—ï¸ Architecture Overview

The application has been restructured into a modular design with clear separation of concerns:

```
app/
â”œâ”€â”€ main.py                 # FastAPI application entry point
â”œâ”€â”€ api/                    # API layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ routes.py          # API endpoints and route handlers
â”œâ”€â”€ config/                 # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py        # Centralized settings with environment variables
â”œâ”€â”€ models/                 # Data models and schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ requests.py        # Request models for API endpoints
â”‚   â”œâ”€â”€ responses.py       # Response models for API endpoints
â”‚   â””â”€â”€ schemas.py         # Core data schemas and Pydantic models
â”œâ”€â”€ services/               # Business logic layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent_service.py   # AI agent for metadata generation
â”‚   â””â”€â”€ profiling_service.py # Comprehensive data profiling with type conversion
â”œâ”€â”€ utils/                  # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_utils.py      # Data processing utilities
â”‚   â”œâ”€â”€ file_utils.py      # File handling utilities
â”‚   â””â”€â”€ validation_utils.py # Validation and parsing utilities
â””â”€â”€ prompts/                # LLM prompt templates
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ metadata_prompts.py # Prompts for metadata generation
    â””â”€â”€ relationship_prompts.py # Prompts for relationship inference
```

## ğŸš€ Features

### Core Features (Implemented)
- **Automated Data Profiling**: Comprehensive analysis of CSV datasets
- **AI-Powered Metadata Generation**: Uses LLM to generate human-readable descriptions
- **Type Conversion Analysis**: Advanced type inference and correction suggestions
- **Relationship Inference**: AI-powered analysis to identify entity relationships
- **RESTful API**: Clean FastAPI endpoints for dataset analysis

### Enhanced Features (New in Modular Version)
- **Centralized Configuration**: Environment-based settings management
- **Comprehensive Type Analysis**: Merged and enhanced type conversion capabilities
- **Modular Services**: Clean separation of profiling and AI services
- **Improved Error Handling**: Better error messages and graceful degradation
- **Extensible Architecture**: Easy to add new features and services

## ğŸ“‹ Requirements

```
fastapi
uvicorn[standard]
pandas
langchain
langchain-openai
langgraph
python-dotenv
numpy
pydantic
pydantic-settings
```

## âš™ï¸ Configuration

Create a `.env` file in the project root:

```env
# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.0

# Application Configuration
APP_TITLE=AI-Powered Logistics Assistant (Phase 1)
APP_VERSION=1.0.0
DEBUG=false

# Data Processing Configuration
NUMERIC_THRESHOLD=0.8
DATETIME_THRESHOLD=0.7
CATEGORICAL_THRESHOLD=50
```

## ğŸš€ Getting Started

1. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up Environment**:
   ```bash
   cp .env.example .env
   # Edit .env with your OpenAI API key
   ```

3. **Run the Application**:
   ```bash
   python -m app.main
   # or
   uvicorn app.main:app --reload
   ```

4. **Access the API**:
   - API Documentation: http://localhost:8000/docs
   - Health Check: http://localhost:8000/health

## ğŸ“š API Usage

### Analyze Dataset
```bash
curl -X POST "http://localhost:8000/v1/analyze" \
     -H "Content-Type: application/json" \
     -d '{"file_path": "datasets/sample_logistics.csv"}'
```

### Finalize Metadata
```bash
curl -X POST "http://localhost:8000/v1/metadata/finalize" \
     -H "Content-Type: application/json" \
     -d '{
       "dataset_name": "sample_logistics.csv",
       "final_metadata": {
         "dataset_description": "Logistics delivery data",
         "columns": [...]
       }
     }'
```

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# Test modular structure
python test_modular_structure.py

# Test API endpoints (requires OpenAI API key)
python test_api.py
```

## ğŸ”„ Migration from Legacy Code

The modular restructure maintains full backward compatibility while providing better organization, enhanced maintainability, improved testing capabilities, and centralized configuration management.

### Key Changes:
- `profiling.py` â†’ `services/profiling_service.py` (enhanced with type conversion)
- `agent.py` â†’ `services/agent_service.py` (improved structure)
- `type_conversion.py` â†’ Merged into `profiling_service.py`
- Added comprehensive models, utilities, and configuration management

## ğŸš€ Phase 2: Natural Language Querying (NEW!)

Phase 2 introduces a sophisticated multi-agent system that allows users to query datasets using natural language. The system uses LangGraph to orchestrate multiple AI agents that work together to understand queries, generate code, execute it safely, and provide comprehensive responses.

### ğŸ¤– Multi-Agent Architecture

The system employs 5 specialized agents:

1. **Planner Agent**: Breaks down user queries into sequential, actionable steps
2. **Code Generation Agent**: Generates Python code using ReAct methodology
3. **Error Analysis Agent**: Diagnoses and suggests fixes for failed code execution
4. **Chart Generation Agent**: Creates Plotly visualizations for results
5. **Final Response Agent**: Synthesizes findings into human-readable summaries

### ğŸ”„ Real-Time Streaming

- **WebSocket Integration**: Real-time streaming of agent thoughts, code generation, and execution results
- **Live Updates**: Users see the AI's reasoning process as it happens
- **Interactive Sessions**: Persistent sessions with Jupyter kernel state management

### ğŸ›¡ï¸ Safe Code Execution

- **Jupyter Sandbox**: Isolated execution environment using jupyter_client
- **Session Management**: Each user gets a dedicated Jupyter kernel
- **State Persistence**: Variables and data persist across queries within a session
- **Error Recovery**: Automatic error analysis and code correction

### ğŸ“Š Advanced Features

- **Parquet Storage**: Efficient data storage and retrieval
- **Chart Generation**: Automatic visualization creation with Plotly
- **Conversation History**: Context-aware responses based on previous interactions
- **Performance Monitoring**: Comprehensive logging and performance tracking

### ğŸ”Œ Phase 2 API Endpoints

#### Start a Query Session
```bash
POST /v2/query
{
  "session_id": "optional-existing-session-id",
  "dataset_name": "sample_logistics.csv",
  "query": "What is the average delivery time by city?",
  "conversation_history": []
}
```

#### WebSocket Streaming
```bash
ws://localhost:8000/v2/stream/{session_id}
```

Send messages:
```json
{
  "type": "query",
  "query": "Show me the top 5 drivers by performance",
  "conversation_history": []
}
```

Receive real-time updates:
```json
{
  "type": "thought",
  "payload": "I need to analyze driver performance metrics...",
  "timestamp": "2025-08-10T14:30:00",
  "step_index": 1
}
```

#### Session Management
```bash
GET /v2/sessions                    # List active sessions
DELETE /v2/sessions/{session_id}    # Cleanup specific session
POST /v2/sessions/cleanup           # Cleanup inactive sessions
```

### ğŸ¯ Example Usage Flow

1. **Start Session**: POST to `/v2/query` with your dataset and question
2. **Connect WebSocket**: Use the returned WebSocket URL
3. **Send Query**: Send your natural language query via WebSocket
4. **Watch Magic**: See real-time agent thoughts, code generation, and execution
5. **Get Results**: Receive final analysis with charts and explanations

## ğŸ“„ Project Structure

```
app/
â”œâ”€â”€ main.py                 # FastAPI application with both phases
â”œâ”€â”€ api/                    # API layer
â”‚   â”œâ”€â”€ routes.py          # Phase 1 endpoints
â”‚   â””â”€â”€ phase2_routes.py   # Phase 2 endpoints
â”œâ”€â”€ config/                 # Configuration management
â”œâ”€â”€ models/                 # Data models and schemas
â”‚   â”œâ”€â”€ requests.py        # Phase 1 models
â”‚   â”œâ”€â”€ responses.py       # Phase 1 models
â”‚   â”œâ”€â”€ schemas.py         # Core schemas
â”‚   â””â”€â”€ phase2_models.py   # Phase 2 models
â”œâ”€â”€ services/               # Business logic layer
â”‚   â”œâ”€â”€ profiling_service.py    # Enhanced profiling with type conversion
â”‚   â”œâ”€â”€ agent_service.py        # Phase 1 AI agent
â”‚   â”œâ”€â”€ session_service.py      # Phase 2 session management
â”‚   â”œâ”€â”€ jupyter_service.py      # Phase 2 code execution
â”‚   â”œâ”€â”€ multi_agent_service.py  # Phase 2 multi-agent system
â”‚   â””â”€â”€ websocket_service.py    # Phase 2 real-time streaming
â”œâ”€â”€ utils/                  # Utility functions
â”‚   â”œâ”€â”€ data_utils.py      # Data processing
â”‚   â”œâ”€â”€ file_utils.py      # File operations
â”‚   â”œâ”€â”€ validation_utils.py # Validation helpers
â”‚   â””â”€â”€ logging_utils.py   # Comprehensive logging
â””â”€â”€ prompts/                # LLM prompt templates
    â”œâ”€â”€ metadata_prompts.py     # Phase 1 prompts
    â”œâ”€â”€ relationship_prompts.py # Phase 1 prompts
    â””â”€â”€ phase2_prompts.py       # Phase 2 agent prompts
```

### ğŸ“ Additional Directories
- `datasets/`: Place CSV files here for analysis
- `metadata/`: Saved finalized metadata JSON files (Phase 1)
- `temp_data/`: Session data storage (Phase 2)
- Legacy files preserved as `*_old.py` for reference
